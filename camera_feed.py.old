import asyncio
import cv2
import math
import os
import subprocess
from importlib.resources import files
from pathlib import Path
from matplotlib import pyplot as plt

from PIL import Image, ImageOps

import mediapipe as mp
from mediapipe.framework.formats import landmark_pb2
from mediapipe.tasks import python
from mediapipe.tasks.python import vision

from ROS2Controller import ROS2Controller

from frame_ble import FrameBle
from frame_msg import FrameMsg, RxPhoto, TxCaptureSettings, TxSprite, TxImageSpriteBlock

# Initialize mediapipe gesture recognition
base_options = python.BaseOptions(model_asset_path='gesture_recognizer.task')
options = vision.GestureRecognizerOptions(base_options=base_options)
recognizer = vision.GestureRecognizer.create_from_options(options)

mp_hands = mp.solutions.hands
mp_drawing = mp.solutions.drawing_utils
mp_drawing_styles = mp.solutions.drawing_styles

DOG_FRAMES_DIR = "dog_frames"
#REMOTE_HOST = "192.168.12.33"  # IP of computer running the camera
REMOTE_HOST = "0.0.0.0"
HTTP_PORT = "9000"

photo_event = asyncio.Event()
photo_event.set()  # Initially "set" means it's OK to display an image  

async def main():
    await check_camera_feed()


async def send_compressed_image_sprite_block(frame: FrameMsg, image_path: str):
    """
    For the specified image, create a compressed TxSprite, split that sprite into strips and send them
    progressively to Frame as an Image Sprite Block
    """
    try:
        sprite = TxSprite.from_indexed_png_bytes(Path(image_path).read_bytes(), compress=True)

        isb = TxImageSpriteBlock(sprite)
        # send the Image Sprite Block header
        await frame.send_message(0x20, isb.pack())
        # then send all the slices
        for spr in isb.sprite_lines:
            await frame.send_message(0x20, spr.pack())
    except Exception as e:
        print(f"Error sending image sprite block: {e}")
        return

async def check_camera_feed():
    frame = FrameMsg()

    try:
        await frame.connect()
        await frame.send_break_signal()

        # Let the user know we're starting
        await frame.send_lua("frame.display.text('Loading...',1,1);frame.display.show();print(1)", await_print=True)

        # debug only: check our current battery level
        batt_mem = await frame.send_lua('print(frame.battery_level() .. " / " .. collectgarbage("count"))', await_print=True)
        print(f"Battery Level/Memory used: {batt_mem}")

        # send the std lua files to Frame that handle data accumulation and camera
        await frame.upload_stdlua_libs(lib_names=['data', 'image_sprite_block'])

    except Exception as e:
        print(f"Error: {e}")
        
    # Send the main lua application from this project to Frame that will run the app
    # to display the text when the messages arrive
    # We rename the file slightly when we copy it, although it isn't necessary
    #await frame.upload_file("lua/camera_sprite_frame_app.lua", "frame_app.lua")
    await frame.upload_file("lua/compressed_prog_sprite_frame_app.lua", "frame_app.lua")
    # attach the print response handler so we can see stdout from Frame Lua print() statements
    # If we assigned this handler before the frameside app was running,
    # any await_print=True commands will echo the acknowledgement byte (e.g. "1"), but if we assign
    # the handler now we'll see any lua exceptions (or stdout print statements)
    frame.attach_print_response_handler()
    
    # "require" the main lua file to run it
    # Note: we can't await_print here because the require() doesn't return - it has a main loop
    await frame.start_frame_app()
    # give Frame a moment to start the frameside app,
    # based on how much work the app does before it's ready to process incoming data
    await asyncio.sleep(0.5)

    # Now that the Frameside app has started there is no need to send snippets of Lua
    # code directly (in fact, we would need to send a break_signal if we wanted to because
    # the main app loop on Frame is running).
    # From this point we do message-passing with first-class types and send_message() (or send_data())
    #rx_photo = RxPhoto()
    #await rx_photo.start()

    # hook up the RxPhoto receiver
    #frame._user_data_response_handler = rx_photo.handle_data

    # give the frame some time for the autoexposure loop to run (50 times; every 0.1s)
    await asyncio.sleep(5.0)

    # start the photo capture loop and the ros2 controlling interface
    running = True  # Variable to control photo capture loop
    ros_controller = ROS2Controller(workspace_path="~/unitree_ws")

    images = []
    results = []

    i = 0  # Counter for photos
    while running:  # Keep capturing photos
        
         # Create async tasks for parallel execution
        #task_gesture = asyncio.create_task(capture_and_recognize_gesture(frame, rx_photo, ros_controller, images, results, i))
        #task_dog_frame = asyncio.create_task(fetch_and_display_dog_frame(frame, ros_controller))

        # Run both tasks in parallel and wait for them both to finish
        #await asyncio.gather(task_gesture, task_dog_frame)
        await fetch_and_display_dog_frame(frame)
        i += 1
        await asyncio.sleep(0.3)  # Small delay

    # stop the photo handler and clean up resources
    display_batch_of_images_with_gestures_and_hand_landmarks(images, results)

    frame.detach_print_response_handler()
    await frame.stop_frame_app()


async def capture_and_recognize_gesture(frame, rx_photo, ros_controller, images, results, i):
    global photo_event  # Use the shared event flag
    ########### TASK ONE START ###########
    # Request the photo capture
    photo_event.clear()  # Signal that we are taking a photo
    capture_settings = TxCaptureSettings(0x0d, resolution=720)
    await frame.send_message(0x0d, capture_settings.pack())

    # get the jpeg bytes as soon as they're ready
    jpeg_bytes = await asyncio.wait_for(rx_photo.queue.get(), timeout=10.0)
    print(f"Received {len(jpeg_bytes)} bytes of jpeg data")

    # save the jpeg bytes to a file
    photo_filename = f"test_photo_{i}.jpg"
    with open(photo_filename, "wb") as f:
        f.write(jpeg_bytes)

    # Allow display of dog frame again
    photo_event.set()  # Signal that we are done with photo capture 

    # Load the photo
    image = mp.Image.create_from_file(photo_filename)

    # Rotate the image
    recognition_result = recognizer.recognize(image)

    if not recognition_result.gestures:
        print(f"No gestures detected in photo {i + 1}")
        ros_controller.publish_to_topic("/active_gesture", "std_msgs/msg/String", '{data: "none"}')
        return

    images.append(image)
    top_gesture = recognition_result.gestures[0][0]
    hand_landmarks = recognition_result.hand_landmarks
    results.append((top_gesture, hand_landmarks))
    ########### TASK ONE END ###########

    # Publish to ROS Topic
    await handle_gesture_command(top_gesture.category_name.lower(), frame, ros_controller)


async def fetch_and_display_dog_frame(frame):
    """Fetches the latest image from the Unitree robot and displays it on Frame."""
    transfer_latest_image_from_robot()  # Fetch the image

    # Display it on Frame
    await display_latest_dog_frame(frame)

# Move this function to handle different gestures
async def handle_gesture_command(gesture, frame, ros_controller):
    """Handles the corresponding action based on detected gesture."""
    GESTURE_COMMANDS = {
        "thumb_up": "up",
        "thumb_down": "down",
        "pointing_up": "forward",
        "victory": "back",
        "iloveyou": "right",
        "open_palm": "left",
        "closed_fist": "hand"
    }
    
    command = GESTURE_COMMANDS.get(gesture, "none")
    print(f"Executing command: {command}")
    ros_controller.publish_to_topic("/active_gesture", "std_msgs/msg/String", f'{{data: "{command}"}}')


async def send_in_chunks(frame: FrameBle, msg_code, payload):
    """Send a large payload in BLE-compatible chunks."""
    max_chunk_size = frame.max_data_payload() - 5  # Maximum BLE payload size is 240
    print(f"Max BLE payload size: {max_chunk_size}")

    total_size = len(payload)  # Total size of the payload
    sent_bytes = 0  # Tracks how many bytes have been sent so far

    while sent_bytes < total_size:
        remaining_bytes = total_size - sent_bytes  # Remaining data to send
        chunk_size = min(max_chunk_size, remaining_bytes)  # Ensure â‰¤ max_chunk_size

        # Extract the next chunk
        chunk = payload[sent_bytes : sent_bytes + chunk_size]

        print(f"Sending chunk: {len(chunk)} bytes (offset: {sent_bytes}/{total_size})")

        # Add the msg_code (as the first byte of the packet) to the chunk
        if sent_bytes == 0:
            # first packet also has total payload length
            chunk_with_msg_code = bytearray([msg_code, total_size >> 8, total_size & 0xFF]) + chunk
        else:
            chunk_with_msg_code = bytearray([msg_code]) + chunk

        # Send the chunk
        await frame.send_data(chunk_with_msg_code)
        sent_bytes += chunk_size

        # Optional: Small delay to avoid overwhelming BLE
        await asyncio.sleep(0.1)

    print("All chunks sent successfully!")


# Fetch last captured image from dog_frames
async def display_latest_dog_frame(f):
    """Displays the last saved image from the local dog_frames directory."""
    try:
        # Get a list of all .jpg files in the directory
        temp_image_path = f"{DOG_FRAMES_DIR}/latest.jpg"

        if not os.path.exists(temp_image_path):
            raise FileNotFoundError(f"Image file not found: {temp_image_path}")

        if os.path.getsize(temp_image_path) < 1000:  # File is too small, likely incomplete
            raise ValueError(f"Downloaded image is too small! ({os.path.getsize(temp_image_path)} bytes)")


        # Resize the image to fit within the display 320x240
        #image = Image.open(temp_image_path)
        #image = ImageOps.exif_transpose(image)  # Correct orientation based on EXIF data
        #image.thumbnail((320, 240), Image.Resampling.LANCZOS)
        #image.save(temp_image_path, "PNG", quality=85)        

        with Image.open(temp_image_path) as img:
            img = ImageOps.exif_transpose(img)
            img.thumbnail((320, 240), Image.Resampling.LANCZOS)
            img.save(temp_image_path, "PNG", quality=85)

        await send_compressed_image_sprite_block(f, temp_image_path)
        print(f"Displaying latest image")
    
    except Exception as e:
        print(f"Error while displaying the image: {e}")


def transfer_latest_image_from_robot():
    """Transfers the latest image from the robot to `dog_frames/` using http."""
    # Use http to copy the latest file
    try:
        # use wget to download the latest image from the robot
        # wget http://192.168.12.33:9000/captured_images/latest.jpg
        subprocess.run(["wget", "-q", "-O", f"{DOG_FRAMES_DIR}/latest.jpg", f"http://{REMOTE_HOST}:{HTTP_PORT}/captured_images/latest.jpg"],
                       check=True)

        # Make sure the file is valid and fully downloaded before proceeding
        if os.path.getsize(f"{DOG_FRAMES_DIR}/latest.jpg") < 1000:  # File too small = likely incomplete
            raise ValueError(f"Downloaded image is too small! ({os.path.getsize(f'{DOG_FRAMES_DIR}/latest.jpg')} bytes)")

    except subprocess.CalledProcessError as e:
        print(f"HTTP transfer failed: {e}")


def display_batch_of_images_with_gestures_and_hand_landmarks(images, results):
    """Displays a batch of images with the gesture category and its score along with the hand landmarks."""
    # Images and labels.
    images = [image.numpy_view() for image in images]
    # rotate images back to original orientation
    images = [cv2.rotate(image, cv2.ROTATE_90_COUNTERCLOCKWISE) for image in images]
    
    gestures = [top_gesture for (top_gesture, _) in results]
    multi_hand_landmarks_list = [multi_hand_landmarks for (_, multi_hand_landmarks) in results]

    # Auto-squaring: this will drop data that does not fit into square or square-ish rectangle.
    rows = int(math.sqrt(len(images)))
    cols = len(images) // rows

    # Size and spacing.
    FIGSIZE = 13.0
    SPACING = 0.1
    subplot=(rows,cols, 1)
    if rows < cols:
        plt.figure(figsize=(FIGSIZE,FIGSIZE/cols*rows))
    else:
        plt.figure(figsize=(FIGSIZE/rows*cols,FIGSIZE))

    # Display gestures and hand landmarks.
    for i, (image, gestures) in enumerate(zip(images[:rows*cols], gestures[:rows*cols])):
        title = f"{gestures.category_name} ({gestures.score:.2f})"
        dynamic_titlesize = FIGSIZE*SPACING/max(rows,cols) * 40 + 3
        annotated_image = image.copy()

        for hand_landmarks in multi_hand_landmarks_list[i]:
          hand_landmarks_proto = landmark_pb2.NormalizedLandmarkList()
          hand_landmarks_proto.landmark.extend([
            landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in hand_landmarks
          ])

          mp_drawing.draw_landmarks(
            annotated_image,
            hand_landmarks_proto,
            mp_hands.HAND_CONNECTIONS,
            mp_drawing_styles.get_default_hand_landmarks_style(),
            mp_drawing_styles.get_default_hand_connections_style())

        subplot = display_one_image(annotated_image, title, subplot, titlesize=dynamic_titlesize)

    # Layout.
    plt.tight_layout()
    plt.subplots_adjust(wspace=SPACING, hspace=SPACING)
    plt.show()


def display_one_image(image, title, subplot, titlesize=16):
    """Displays one image along with the predicted category name and score."""
    plt.subplot(*subplot)
    plt.imshow(image)
    if len(title) > 0:
        plt.title(title, fontsize=int(titlesize), color='black', fontdict={'verticalalignment':'center'}, pad=int(titlesize/1.5))
    return (subplot[0], subplot[1], subplot[2]+1)


def rotate_image(image, angle):
    """Rotates a given Mediapie image by a specified angle."""
    # If the input image is in the form of a NumPy array, process it directly
    if image is None:
        raise ValueError("Input image is None. Please provide a valid image.")

    # Get height and width of mediapipe image
    h, w, _ = image.numpy_view().shape

    # Compute the center of the image
    center = (w // 2, h // 2)

    # Create a rotation matrix
    M = cv2.getRotationMatrix2D(center, angle, scale=1.0)

    # Perform the rotation for mp.Image
    rotated_image_np = cv2.warpAffine(image.numpy_view(), M, (w, h))

    rotated_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rotated_image_np)

    # Return the rotated image as a NumPy array
    return rotated_image


if __name__ == "__main__":
    asyncio.run(main())
